{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Fitting \"in-the-wild\" videos\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "- Access to a combined 3D facial shape model of identity and expression\n",
    "- Access to an \"in-the-wild\" texture model\n",
    "- Access to an \"in-the-wild\" video with landmarks to fit. The per-frame landmarks should be given as input and should follow the iBUG 68 mark-up. It is highly recommended that these landmarks are \"3D-Aware-2D\" (3DA-2D) landmarks, as e.g. described in the paper (Zafeiriou et al., \"The 3D menpo facial landmark tracking challenge\", ICCV-W 2017).\n",
    "\n",
    "The first two of these can be generated by following the two previous notebooks in this folder:\n",
    "\n",
    "- `1. Building an \"in-the-wild\" texture model.ipynb`\n",
    "- `2. Creating an expressive 3DMM.ipynb`\n",
    "\n",
    "Running these notebooks to completion will lead to the following two files being generated in the `DATA_DIR` folder:\n",
    "\n",
    "- `itw_texture_model.pkl`\n",
    "- `id_exp_shape_model.pkl`\n",
    "\n",
    "This script shows how to load these directly and use them in fitting.\n",
    "\n",
    "You could of course understand the required formats by studying the aformentinoed scripts, and instead load your own shape and texture models instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import menpo.io as mio\n",
    "from menpo.base import LazyList\n",
    "from menpo.shape import PointCloud\n",
    "from menpo.visualize import print_progress\n",
    "from menpo3d.morphablemodel import ColouredMorphableModel\n",
    "\n",
    "from itwmm import (\n",
    "    initialize_camera_from_params, initialize_camera,\n",
    "    fit_video, instance_for_params,\n",
    "    render_initialization, render_iteration,\n",
    ")\n",
    "\n",
    "# Replace DATA_PATH with the path to your data. It should have files:\n",
    "#  itw_texture_model.pkl\n",
    "#  id_exp_shape_model.pkl\n",
    "# As generated from Notebooks 1. and 2.)\n",
    "DATA_PATH = Path('~/Dropbox/itwmm_src_data/').expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_image_and_return_transforms(diagonal, feature_f, image):\n",
    "    # this variation is needed if we need to know how the imput image \n",
    "    # is transformed\n",
    "    img, t = image.crop_to_landmarks_proportion(0.4, return_transform=True)\n",
    "    img, scale = img.rescale_landmarks_to_diagonal_range(diagonal, return_transform=True)\n",
    "    return {\n",
    "        'image': feature_f(img),\n",
    "        't': t.translation_component,\n",
    "        'scale': scale.scale[0]\n",
    "    }\n",
    "\n",
    "\n",
    "def load_id_exp_shape_model(path):\n",
    "    sm_dict = mio.import_pickle(path)\n",
    "    shape_model = sm_dict['shape_model']\n",
    "    lms = sm_dict['lms']\n",
    "    id_ind = sm_dict['id_ind']\n",
    "    exp_ind = sm_dict['exp_ind']\n",
    "    return shape_model, lms, id_ind, exp_ind\n",
    "\n",
    "\n",
    "def load_itw_texture_model(path):\n",
    "    tm_dict = mio.import_pickle(path)\n",
    "    texture_model  = tm_dict['texture_model']\n",
    "    diagonal_range = tm_dict['diagonal_range']\n",
    "    feature_function = tm_dict['feature_function']\n",
    "    return texture_model, diagonal_range, feature_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD SHAPE MODEL\n",
    "# note that id_ind and exp_ind are two index mappings into the components of\n",
    "# this special combined shape model. The first records the index position of\n",
    "# components that are related to identitiy, the second an index of the (remaining)\n",
    "# components which are related to shape.\n",
    "shape_model, lms, id_ind, exp_ind = load_id_exp_shape_model(DATA_PATH / 'id_exp_shape_model.pkl')\n",
    "\n",
    "# record the number of ID / EXP params\n",
    "n_p, n_q = id_ind.shape[0], exp_ind.shape[0]\n",
    "\n",
    "# LOAD ITW TEXTURE MODEL\n",
    "# Note we have to know the diagonal setting and feature used in the texture model.\n",
    "texture_model, diagonal_range, feature_function = load_itw_texture_model(DATA_PATH / 'itw_texture_model.pkl')\n",
    "\n",
    "# construct our Morphable Model that we can use in the fitting approaches below\n",
    "mm = ColouredMorphableModel(shape_model, texture_model, lms, \n",
    "                            holistic_features=feature_function,\n",
    "                            diagonal=diagonal_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some images and prepare them for fitting.\n",
    "# Note that we have to rescale the images/extract the feature we used for the model\n",
    "# ourselves. Unlike previous menpo fitting routines, fit_video is a simpler implementation.\n",
    "# it requires us to explicitly do more before we call fit_video, but it is much simpler to\n",
    "# follow what is being done in the code.\n",
    "lim_frames = 100\n",
    "\n",
    "frame_ids = LazyList.init_from_iterable(\n",
    "    [p.stem for p in mio.image_paths('video_dir/')][:lim_frames])\n",
    "frames = frame_ids.map(lambda fid: mio.import_image('video_dir/{}.png'.format(fid)))\n",
    "\n",
    "transform_info = [prepare_image_and_return_transforms(diagonal_range, feature_function, i) for i in \n",
    "                  print_progress(frames, prefix='processing images')]\n",
    "images = [x.pop('image') for x in transform_info]\n",
    "\n",
    "n_images = len(images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# initialize the shape weights to zero (mean)\n",
    "p = np.zeros(n_p)\n",
    "qs = np.zeros([n_images, n_q])\n",
    "\n",
    "# initialize all cameras with a large focal length (orthogathic)\n",
    "cameras = [initialize_camera_from_params(img, mm, id_ind, exp_ind, p, q, focal_length=99999999) \n",
    "           for img, q in zip(images, qs)]\n",
    "cs = np.vstack([camera.as_vector() for camera in cameras])\n",
    "template_camera = cameras[0]\n",
    "\n",
    "# Check the initialization looks sensible (for first and last frame)\n",
    "render_initialization(images, mm, id_ind, exp_ind, template_camera, p, qs, cs, 0).view()\n",
    "render_initialization(images, mm, id_ind, exp_ind, template_camera, p, qs, cs, -1).view(new_figure=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ITW Video fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually run the optimisation.\n",
    "# Return is a list of parameters recovered per-iteration per-frame.\n",
    "#\n",
    "# E.g. to access the 3'rd frame's parameters at the 6th iteration:\n",
    "#   params[5][2]  # (lists are 0-based in Python)\n",
    "#\n",
    "params = fit_video(images, mm, id_ind, exp_ind, template_camera, \n",
    "                   p, qs, cs,\n",
    "                   lm_group='PTS', n_iters=10, \n",
    "                   c_f=3.,\n",
    "                   c_id=1. * n_images,\n",
    "                   c_exp=3.,\n",
    "                   c_l=1.,\n",
    "                   c_sm=6.,\n",
    "                   n_samples=1000, compute_costs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we render the fitting.\n",
    "frame_no = 5\n",
    "iter_no = -1\n",
    "frames[frame_no].view()\n",
    "render_iteration(mm, id_ind, exp_ind, images[0].shape, \n",
    "                 template_camera, params, frame_no, iter_no).view(new_figure=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
