{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Building an \"in-the-wild\" texture model\n",
    "\n",
    "#### Prerequisites\n",
    "\n",
    "A collection of \"in-the-wild\" images with 3D mesh fits.\n",
    "\n",
    "The authors of:\n",
    "\n",
    "> Zhu, Xiangyu, et al. \"Face alignment across large poses: A 3d solution.\" CVPR 2016\n",
    "\n",
    "Provide data [on their website](http://www.cbsr.ia.ac.cn/users/xiangyuzhu/projects/3DDFA/main.htm) which can be used to bootstrap this. As an example, we show loading this data into the format we need to proceed, and then demonstrate building the texture model. The two files this example follows are:\n",
    "```\n",
    "300W-3D.zip\n",
    "300W-3D-Face.zip\n",
    "```\n",
    "unzip them next to each other in a folder and replace `DATA_PATH` with the parent folder path. \n",
    "\n",
    "If you have your own collection of \"in-the-wild\" images, you can easily replace this first cell with your own loading code - as long as you load each image with a 3D `TriMesh` that menpo visualizes the 2D projection of correctly, you are good. A good sanity check is therefore:\n",
    "```\n",
    "%matplotlib inline\n",
    "img.view()\n",
    "PointCloud(fit_3d.points[:, :2]).view()\n",
    "```\n",
    "You should see that Menpo renders the points of the mesh on the face in the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data\n",
    "\n",
    "Firstly we load the data we need in the right format. We need to produce:\n",
    "\n",
    "- A list of tuples of `(feature_img, fit_trimesh_3d)`, where\n",
    "  - `feature_img` has already had a dense image feature e.g. `fast_dist` extracted from it\n",
    "  - All instances of `feature_img` are scaled such that the face occupies the same size in each image\n",
    "  - `fit_trimesh_3d` lies directly on the dense feature image - i.e. Y, X are in units of pixels, and Z is of an arbitrary scale (so long as depth is meaningfully conveyed for z-buffering)\n",
    "  - A good test for this last condition is to visualize the mesh using Menpo (see the example below)\n",
    "  \n",
    "In order to be memory efficient, we choose to construct a `LazyList` rather than a literal `list` here. This means each feature/fit pair is generated from disk on the fly when we access each element, keeping memory usage low."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io as sio\n",
    "import menpo.io as mio\n",
    "\n",
    "from menpo.base import LazyList\n",
    "from menpo.feature import fast_dsift\n",
    "from menpo.shape import PointCloud, TriMesh\n",
    "\n",
    "from itwmm import (render_overlay_of_mesh_in_img, \n",
    "                   generate_texture_model_from_image_3d_fits)\n",
    "\n",
    "# Replace DATA_PATH with the path to your data. It should have subdirectories:\n",
    "#  300W-3D/\n",
    "#  300W-3D-Face/\n",
    "DATA_PATH = Path('~/Dropbox/itwmm_src_data/').expanduser()\n",
    "\n",
    "# The diagonal range of the projected fit\n",
    "DIAGONAL_RANGE = 180\n",
    "\n",
    "# The image feature we wish to employ\n",
    "FEATURE_F = fast_dsift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@lru_cache(1)\n",
    "def load_trilist():\n",
    "    trilist = sio.loadmat(str(DATA_PATH / '300W-3D' / 'Code' / 'ModelGeneration' / 'model_info.mat'))['tri']\n",
    "    # flip triangle faces and matlab -> Python indexing\n",
    "    return trilist[[1, 0, 2]].T - 1     \n",
    "\n",
    "\n",
    "def convert_raw_3d_fit_to_img_trimesh_fit(raw_3d_fit, img):\n",
    "    img_points_2d = raw_3d_fit[:2][::-1].copy() - 1  # matlab 1 indexing\n",
    "    img_points_2d[0] = img.shape[0] - img_points_2d[0]\n",
    "    # Depth needs to be flipped\n",
    "    img_points_3d = np.vstack([img_points_2d, raw_3d_fit[2] * -1]).T\n",
    "    img_trimesh_3d = TriMesh(img_points_3d, trilist=load_trilist())\n",
    "    return img_trimesh_3d\n",
    "\n",
    "\n",
    "def load_img_and_fit(img_path):\n",
    "    fit_rel_path = img_path.relative_to(DATA_PATH / '300W-3D').with_suffix('.mat')\n",
    "    fit_path = DATA_PATH / '300W-3D-Face' / fit_rel_path\n",
    "\n",
    "    img = mio.import_image(img_path).as_greyscale()\n",
    "    raw_3d_fit = sio.loadmat(str(fit_path))['Fitted_Face']\n",
    "    fit_trimesh_3d = convert_raw_3d_fit_to_img_trimesh_fit(raw_3d_fit, img)\n",
    "    return img, fit_trimesh_3d\n",
    "\n",
    "\n",
    "def load_data_with_feature_sample(img_path, err_proportion=0.0001):\n",
    "    img, fit_trimesh_3d = load_img_and_fit(img_path)\n",
    "    \n",
    "    # rescale the image to the diagonal range we are using\n",
    "    img.landmarks['fit_2d'] = PointCloud(fit_trimesh_3d.points[:, :2])\n",
    "    img, tr = img.rescale_landmarks_to_diagonal_range(DIAGONAL_RANGE, return_transform=True)\n",
    "    # note that we use the rescaled landmarks here\n",
    "    fit_trimesh_3d.points[:, :2] = img.landmarks['fit_2d'].points\n",
    "    fit_trimesh_3d.points[:, 2] = fit_trimesh_3d.points[:, 2] / tr.scale[0]\n",
    "    # take the feature on the rescaled image\n",
    "    feat = FEATURE_F(img)\n",
    "    return img, feat, fit_trimesh_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_paths = list(mio.image_paths(DATA_PATH / '300W-3D' / '**/*'))\n",
    "print('{} images with fits to read.'.format(len(img_paths)))\n",
    "\n",
    "# we can make a LazyList which returns image/feature/fit triples when\n",
    "# we iterate. This saves us loading everything into memory at once.\n",
    "imgs_features_and_fits = LazyList.init_from_iterable(img_paths).map(\n",
    "    load_data_with_feature_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Let's test one.\n",
    "img, feat, fit_trimesh_3d = imgs_features_and_fits[100]\n",
    "\n",
    "# Visual check that everything looks sensible\n",
    "img.view()\n",
    "feat.view(channels=0, new_figure=True)\n",
    "render_overlay_of_mesh_in_img(fit_trimesh_3d, img).view(new_figure=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the routine to build the ITW texture model only requires \n",
    "# feat/fit pairs, so we get rid of the redundent images\n",
    "features_and_fits = imgs_features_and_fits.map(lambda x: x[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm we get the format we need\n",
    "print(features_and_fits[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building the ITW texture model\n",
    "\n",
    "Now we are able to build the texture model. Given data in the right format, this is just a single function call.\n",
    "\n",
    "Note however that this process is quite memory intensive. We've done all we can in loading the data lazily, we still need to constract a large data matrix X and a mask matrix, and solve RPCA which has some memory overhead.\n",
    "\n",
    "As a rough guide, with the recommended settings here, around 1.8GB of RAM will be required for every 100 images used.\n",
    "\n",
    "Feel free to tweak `n_imgs_for_rpca` and choose wheather to use the original 64-bit feature images or 32-bit conversions (which will half RAM requirements).\n",
    "\n",
    "Also a deprecation warning will appear in red, but it's only a notice of a future deprecation - this will be addressed in a future release of `menpo3d`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def as_float32(img):\n",
    "    img = img.copy()\n",
    "    img.pixels = img.pixels.astype(np.float32)\n",
    "    return img\n",
    "\n",
    "features_and_fits_32bit = features_and_fits.map(lambda x: (as_float32(x[0]), x[1]))\n",
    "n_imgs_for_rpca = 100\n",
    "\n",
    "itw_texture_model, X, m = generate_texture_model_from_image_3d_fits(features_and_fits_32bit[:n_imgs_for_rpca])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(itw_texture_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The matrices used for R-PCA were of shape: {} & {}'.format(X.shape, m.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "from itwmm.base import as_colouredtrimesh\n",
    "# As a sanity check, visualize the masks found on the original geometry\n",
    "# with the texture value extracted (mask = red)\n",
    "i = 4\n",
    "img, feat, fit_trimesh_3d = imgs_features_and_fits[i]\n",
    "img_sampled = np.repeat(img.sample(PointCloud(fit_trimesh_3d.points[:, :2])), 3, axis=0).T\n",
    "mask = m[i].reshape([fit_trimesh_3d.n_points, -1])[:, 0]\n",
    "img_sampled[~mask, 0] = 1.\n",
    "as_colouredtrimesh(fit_trimesh_3d, colours=img_sampled).view()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exporting a ITW texture model\n",
    "\n",
    "To use this model we just need to save it out and load it in to the fitting notebooks. Note that we need to save out some metadata along with the raw PCA basis to be able to use this again in the future. In particular, we need:\n",
    "\n",
    "- The rescaling diagonal. We will need this at fit time to ensure the features are meaningful.\n",
    "- The feature extraction function. Again we need to call this at fit time to produce a meaningful feature image to compare against in our LK algorithm.\n",
    "\n",
    "We also need the shape model we use to be in perfect correspondence with the fits used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mio.export_pickle(\n",
    "    {\n",
    "        'texture_model': itw_texture_model,\n",
    "        'diagonal_range': 180,\n",
    "        'feature_function': fast_dsift\n",
    "    }, \n",
    "    DATA_PATH / 'itw_texture_model.pkl',\n",
    "    protocol=4, overwrite=True)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
